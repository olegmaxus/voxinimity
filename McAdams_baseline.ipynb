{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f460cc4-95d7-4f0b-b698-638d4e30959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9497e48-7b98-432c-b317-66fa62dcf78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 03:07:22.222822: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 03:07:22.578637: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-19 03:07:22.578701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-19 03:07:22.638346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-19 03:07:22.760648: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 03:07:24.204897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/olegmaxus/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import argparse\n",
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC, pipeline, Speech2TextForConditionalGeneration, Speech2TextProcessor\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Inference, Model\n",
    "import numpy as np\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "from scipy.spatial.distance import cdist\n",
    "from pyannote.audio import Model\n",
    "from torcheval.metrics import WordErrorRate\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb80bdbc-a6e6-48bf-b603-88d97ed5b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torcheval\n",
    "# !pip install SentencePiece\n",
    "# !pip install datasets transformers[sentencepiece]\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eea9d-d729-4d18-b0d0-62dc2403433d",
   "metadata": {},
   "source": [
    "## McAdams LPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040d022e-add8-441c-9869-61b439fe2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.signal\n",
    "import scipy.io.wavfile\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "class McAdamsLPC:\n",
    "    def __init__(self, window_ms=20, shift_ms=10, lp_order=20, mcadams_coeff=0.8):\n",
    "        self.window_ms = window_ms\n",
    "        self.shift_ms = shift_ms\n",
    "        self.lp_order = lp_order\n",
    "        self.mcadams_coeff = mcadams_coeff\n",
    "\n",
    "    def load_audio_file(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"{file_path} not found.\")\n",
    "        return librosa.load(file_path, sr=None)\n",
    "\n",
    "    def calculate_window_parameters(self, sample_rate):\n",
    "        window_len_samples = int(np.floor(self.window_ms * 0.001 * sample_rate))\n",
    "        shift_samples = int(np.floor(self.shift_ms * 0.001 * sample_rate))\n",
    "        return window_len_samples, shift_samples\n",
    "\n",
    "    def process_signal(self, signal, sample_rate, window_len_samples, shift_samples):\n",
    "        num_samples = len(signal)\n",
    "        num_frames = 1 + (num_samples - window_len_samples) // shift_samples\n",
    "        signal_rec = np.zeros(num_samples)\n",
    "        for frame_num in range(1, num_frames):\n",
    "            frame, out_index = self.extract_frame(signal, frame_num, window_len_samples, shift_samples)\n",
    "            frame_rec = self.process_frame(frame, sample_rate)\n",
    "            signal_rec[out_index] += frame_rec\n",
    "        return signal_rec / np.max(np.abs(signal_rec))\n",
    "\n",
    "    def extract_frame(self, signal, frame_num, window_len_samples, shift_samples):\n",
    "        start = frame_num * shift_samples\n",
    "        end = min(start + window_len_samples, len(signal))\n",
    "        frame = signal[start:end] * np.hanning(window_len_samples)\n",
    "        return frame, np.arange(start, start + len(frame))\n",
    "\n",
    "    def process_frame(self, frame, sample_rate):\n",
    "        eps = np.finfo(np.float32).eps\n",
    "        frame += eps\n",
    "        a_lpc = librosa.lpc(frame, order=self.lp_order)\n",
    "        poles = scipy.signal.tf2zpk([1], a_lpc)[1]\n",
    "        ind_imag = np.where(~np.isreal(poles))[0][::2]\n",
    "        new_poles = self.apply_mcadams(poles, ind_imag)\n",
    "        a_lpc_new = np.real(np.poly(new_poles))\n",
    "        res = scipy.signal.lfilter(a_lpc, [1], frame)\n",
    "        return scipy.signal.lfilter([1], a_lpc_new, res) * np.hanning(len(frame))\n",
    "\n",
    "    def apply_mcadams(self, poles, ind_imag_con):\n",
    "        new_angles = np.angle(poles[ind_imag_con]) ** self.mcadams_coeff\n",
    "        new_angles = np.clip(new_angles, 0, np.pi)\n",
    "        new_poles = np.copy(poles)\n",
    "        for i, idx in enumerate(ind_imag_con):\n",
    "            r = np.abs(poles[idx])\n",
    "            new_poles[idx] = r * np.exp(1j * new_angles[i])\n",
    "            new_poles[idx + 1] = r * np.exp(-1j * new_angles[i])\n",
    "        return new_poles\n",
    "\n",
    "    def denoise_signal(self, input_file, output_file, n_fft=2048, hop_length=512, win_length=2048):\n",
    "        signal, sr = librosa.load(input_file, sr=None)\n",
    "        stft_signal = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "        noise_amp = np.median(np.abs(stft_signal), axis=1, keepdims=True)\n",
    "        spectral_gate = np.mean(noise_amp) * 1.5\n",
    "        stft_signal_denoised = np.where(np.abs(stft_signal) > spectral_gate, stft_signal, 0)\n",
    "        signal_denoised = librosa.istft(stft_signal_denoised, hop_length=hop_length, win_length=win_length)\n",
    "        signal_denoised_normalized = signal_denoised / np.max(np.abs(signal_denoised))\n",
    "        scipy.io.wavfile.write(output_file, sr, np.float32(signal_denoised_normalized))\n",
    "\n",
    "    @classmethod\n",
    "    def anonymize_dataset(cls, dataset_name, dataset_config, split, output_directory,\n",
    "                          anonymization_params={}, denoising_params={}):\n",
    "        ds = load_dataset(dataset_name, dataset_config, split=split)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        anon_instances = []\n",
    "        for i, sample in enumerate(ds):\n",
    "            file_path = sample[\"file\"]\n",
    "            output_file_path = os.path.join(output_directory, f\"anonymized_denoised_{i}.wav\")\n",
    "            mcadams_instance = cls(**anonymization_params)\n",
    "            signal, sample_rate = mcadams_instance.load_audio_file(file_path)\n",
    "            window_len_samples, shift_samples = mcadams_instance.calculate_window_parameters(sample_rate)\n",
    "            anonymized_signal = mcadams_instance.process_signal(signal, sample_rate, window_len_samples, shift_samples)\n",
    "            temp_output_path = output_file_path.replace('.wav', '_temp_anonymized.wav')\n",
    "            mcadams_instance.save_signal(temp_output_path, anonymized_signal, sample_rate)\n",
    "            mcadams_instance.denoise_signal(temp_output_path, output_file_path, **denoising_params)\n",
    "            os.remove(temp_output_path)\n",
    "            anon_instances.append(output_file_path)\n",
    "            print(f\"Processed file {i+1}/{len(ds)}: {output_file_path}\")\n",
    "        return anon_instances, ds\n",
    "\n",
    "    def save_signal(self, file_path, signal, sample_rate):\n",
    "        normalized_signal = signal / np.max(np.abs(signal))\n",
    "        scipy.io.wavfile.write(file_path, sample_rate, np.float32(normalized_signal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7f8bd8-d9b8-4cc3-8e45-5535c85334ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 1/73: ./anonymized_dataset/anonymized_denoised_0.wav\n",
      "Processed file 2/73: ./anonymized_dataset/anonymized_denoised_1.wav\n",
      "Processed file 3/73: ./anonymized_dataset/anonymized_denoised_2.wav\n",
      "Processed file 4/73: ./anonymized_dataset/anonymized_denoised_3.wav\n",
      "Processed file 5/73: ./anonymized_dataset/anonymized_denoised_4.wav\n",
      "Processed file 6/73: ./anonymized_dataset/anonymized_denoised_5.wav\n",
      "Processed file 7/73: ./anonymized_dataset/anonymized_denoised_6.wav\n",
      "Processed file 8/73: ./anonymized_dataset/anonymized_denoised_7.wav\n",
      "Processed file 9/73: ./anonymized_dataset/anonymized_denoised_8.wav\n",
      "Processed file 10/73: ./anonymized_dataset/anonymized_denoised_9.wav\n",
      "Processed file 11/73: ./anonymized_dataset/anonymized_denoised_10.wav\n",
      "Processed file 12/73: ./anonymized_dataset/anonymized_denoised_11.wav\n",
      "Processed file 13/73: ./anonymized_dataset/anonymized_denoised_12.wav\n",
      "Processed file 14/73: ./anonymized_dataset/anonymized_denoised_13.wav\n",
      "Processed file 15/73: ./anonymized_dataset/anonymized_denoised_14.wav\n",
      "Processed file 16/73: ./anonymized_dataset/anonymized_denoised_15.wav\n",
      "Processed file 17/73: ./anonymized_dataset/anonymized_denoised_16.wav\n",
      "Processed file 18/73: ./anonymized_dataset/anonymized_denoised_17.wav\n",
      "Processed file 19/73: ./anonymized_dataset/anonymized_denoised_18.wav\n",
      "Processed file 20/73: ./anonymized_dataset/anonymized_denoised_19.wav\n",
      "Processed file 21/73: ./anonymized_dataset/anonymized_denoised_20.wav\n",
      "Processed file 22/73: ./anonymized_dataset/anonymized_denoised_21.wav\n",
      "Processed file 23/73: ./anonymized_dataset/anonymized_denoised_22.wav\n",
      "Processed file 24/73: ./anonymized_dataset/anonymized_denoised_23.wav\n",
      "Processed file 25/73: ./anonymized_dataset/anonymized_denoised_24.wav\n",
      "Processed file 26/73: ./anonymized_dataset/anonymized_denoised_25.wav\n",
      "Processed file 27/73: ./anonymized_dataset/anonymized_denoised_26.wav\n",
      "Processed file 28/73: ./anonymized_dataset/anonymized_denoised_27.wav\n",
      "Processed file 29/73: ./anonymized_dataset/anonymized_denoised_28.wav\n",
      "Processed file 30/73: ./anonymized_dataset/anonymized_denoised_29.wav\n",
      "Processed file 31/73: ./anonymized_dataset/anonymized_denoised_30.wav\n",
      "Processed file 32/73: ./anonymized_dataset/anonymized_denoised_31.wav\n",
      "Processed file 33/73: ./anonymized_dataset/anonymized_denoised_32.wav\n",
      "Processed file 34/73: ./anonymized_dataset/anonymized_denoised_33.wav\n",
      "Processed file 35/73: ./anonymized_dataset/anonymized_denoised_34.wav\n",
      "Processed file 36/73: ./anonymized_dataset/anonymized_denoised_35.wav\n",
      "Processed file 37/73: ./anonymized_dataset/anonymized_denoised_36.wav\n",
      "Processed file 38/73: ./anonymized_dataset/anonymized_denoised_37.wav\n",
      "Processed file 39/73: ./anonymized_dataset/anonymized_denoised_38.wav\n",
      "Processed file 40/73: ./anonymized_dataset/anonymized_denoised_39.wav\n",
      "Processed file 41/73: ./anonymized_dataset/anonymized_denoised_40.wav\n",
      "Processed file 42/73: ./anonymized_dataset/anonymized_denoised_41.wav\n",
      "Processed file 43/73: ./anonymized_dataset/anonymized_denoised_42.wav\n",
      "Processed file 44/73: ./anonymized_dataset/anonymized_denoised_43.wav\n",
      "Processed file 45/73: ./anonymized_dataset/anonymized_denoised_44.wav\n",
      "Processed file 46/73: ./anonymized_dataset/anonymized_denoised_45.wav\n",
      "Processed file 47/73: ./anonymized_dataset/anonymized_denoised_46.wav\n",
      "Processed file 48/73: ./anonymized_dataset/anonymized_denoised_47.wav\n",
      "Processed file 49/73: ./anonymized_dataset/anonymized_denoised_48.wav\n",
      "Processed file 50/73: ./anonymized_dataset/anonymized_denoised_49.wav\n",
      "Processed file 51/73: ./anonymized_dataset/anonymized_denoised_50.wav\n",
      "Processed file 52/73: ./anonymized_dataset/anonymized_denoised_51.wav\n",
      "Processed file 53/73: ./anonymized_dataset/anonymized_denoised_52.wav\n",
      "Processed file 54/73: ./anonymized_dataset/anonymized_denoised_53.wav\n",
      "Processed file 55/73: ./anonymized_dataset/anonymized_denoised_54.wav\n",
      "Processed file 56/73: ./anonymized_dataset/anonymized_denoised_55.wav\n",
      "Processed file 57/73: ./anonymized_dataset/anonymized_denoised_56.wav\n",
      "Processed file 58/73: ./anonymized_dataset/anonymized_denoised_57.wav\n",
      "Processed file 59/73: ./anonymized_dataset/anonymized_denoised_58.wav\n",
      "Processed file 60/73: ./anonymized_dataset/anonymized_denoised_59.wav\n",
      "Processed file 61/73: ./anonymized_dataset/anonymized_denoised_60.wav\n",
      "Processed file 62/73: ./anonymized_dataset/anonymized_denoised_61.wav\n",
      "Processed file 63/73: ./anonymized_dataset/anonymized_denoised_62.wav\n",
      "Processed file 64/73: ./anonymized_dataset/anonymized_denoised_63.wav\n",
      "Processed file 65/73: ./anonymized_dataset/anonymized_denoised_64.wav\n",
      "Processed file 66/73: ./anonymized_dataset/anonymized_denoised_65.wav\n",
      "Processed file 67/73: ./anonymized_dataset/anonymized_denoised_66.wav\n",
      "Processed file 68/73: ./anonymized_dataset/anonymized_denoised_67.wav\n",
      "Processed file 69/73: ./anonymized_dataset/anonymized_denoised_68.wav\n",
      "Processed file 70/73: ./anonymized_dataset/anonymized_denoised_69.wav\n",
      "Processed file 71/73: ./anonymized_dataset/anonymized_denoised_70.wav\n",
      "Processed file 72/73: ./anonymized_dataset/anonymized_denoised_71.wav\n",
      "Processed file 73/73: ./anonymized_dataset/anonymized_denoised_72.wav\n"
     ]
    }
   ],
   "source": [
    "mcd = McAdamsLPC()\n",
    "all_names, ds = mcd.anonymize_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", \"validation\", \"./anonymized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed999f1-40e1-476c-a8bc-1cd70e80d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    's2t': {\n",
    "        'model_name': 'facebook/s2t-small-librispeech-asr' \n",
    "    },\n",
    "    'hf_token': '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',\n",
    "    'save_dir': './audio'\n",
    "}\n",
    "\n",
    "# s2t-small-librispeech-asr\n",
    "_config = config['s2t']\n",
    "\n",
    "#STT\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(_config['model_name'])\n",
    "processor = Speech2TextProcessor.from_pretrained(_config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1acd074e-04a9-48d4-bfad-30a188b1f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer( orig_src, anon_src):\n",
    "    model_emb = Model.from_pretrained(\"pyannote/embedding\", use_auth_token='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')\n",
    "    inference = Inference(model_emb, window=\"whole\")\n",
    "    \n",
    "    orig_embeddings = [\n",
    "        inference(f) for f in orig_src    \n",
    "    ]\n",
    "\n",
    "    anon_embeddings = [\n",
    "        inference(f) for f in anon_src \n",
    "    ]\n",
    "\n",
    "    emb_list = orig_embeddings + anon_embeddings\n",
    "    label_list = [1]*len(orig_embeddings) + [-1]*len(anon_embeddings)\n",
    "\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    \n",
    "    for emb1, label1 in zip(emb_list, label_list):\n",
    "        for emb2, label2 in zip(emb_list, label_list):\n",
    "            distance = cdist(emb1.reshape(1,-1), emb2.reshape(1,-1), metric=\"cosine\")[0,0]\n",
    "            score = max(0, 1-distance)\n",
    "            if label1!=label2:\n",
    "                negative_scores.append(score)\n",
    "            else:\n",
    "                positive_scores.append(score)\n",
    "\n",
    "    #print(positive_scores, negative_scores)\n",
    "    val_eer, threshold = EER(torch.tensor(positive_scores), torch.tensor(negative_scores))\n",
    "\n",
    "    return val_eer\n",
    "\n",
    "def compute_wer( orig_texts, anon_paths ):\n",
    "    metric = WordErrorRate()\n",
    "\n",
    "    #Load and stt anon\n",
    "    anon_arrs = []\n",
    "    anon_rates = []\n",
    "    for p in anon_paths:\n",
    "        arr, rate = sf.read(p)\n",
    "        anon_arrs.append(arr)\n",
    "        anon_rates.append(rate)\n",
    "    inputs = processor(anon_arrs, sampling_rate=anon_rates[0], return_tensors=\"pt\", padding=True)\n",
    "    generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    anon_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for org, an in zip(orig_texts, anon_texts):\n",
    "        metric.update([an.lower()], [org.lower()])\n",
    "\n",
    "    return metric.compute().item(), orig_texts, anon_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df74ddc-ca9a-48a8-a0fb-1fd2fc10d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_eer(\n",
    "    [a['path'] for a in ds[\"audio\"]], \n",
    "    all_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce0ef48-1eb1-4127-97a6-2aac3bb4edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, true_ones, anon_ones = compute_wer(ds['text'], all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d8141d-d306-4c3d-ba11-7d2db2b4782b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18347826600074768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d7cb1-bb22-45c4-a51b-0a519daf77e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
