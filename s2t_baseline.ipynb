{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "75261fd0-44d5-4f19-b94d-9843a733eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "import librosa as lb\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Inference, Model\n",
    "import numpy as np\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "from scipy.spatial.distance import cdist\n",
    "from pyannote.audio import Model\n",
    "from torcheval.metrics import WordErrorRate\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4db875fd-ea43-45ac-b9b4-2cfc35563c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyannote.audio\n",
    "#!pip install speechbrain\n",
    "#!pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7807d79d-1c71-4109-b4c7-c45c90d9ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    's2t': {\n",
    "        'model_name': 'facebook/s2t-small-librispeech-asr'\n",
    "    },\n",
    "    'hf_token': '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',\n",
    "    'save_dir': './audio'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481ded82-9eb4-4842-9075-1dac96334575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d4c614b7-5f60-4296-a5f7-ede21d796beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\razvor\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\c6335d8f1cd77b30084387468a6cf26fea90009b\\pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\razvor\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\c6335d8f1cd77b30084387468a6cf26fea90009b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.1.2. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.1.2. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "_config = config['s2t']\n",
    "\n",
    "#STT\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(_config['model_name'])\n",
    "processor = Speech2TextProcessor.from_pretrained(_config['model_name'])\n",
    "\n",
    "#TTS\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "#Speaker emb\n",
    "model_emb = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=config['hf_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7346e4b-d647-4724-b2a6-489b8a9f8a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7feb95c3-fc78-4c2f-bf4a-6d54e583ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonimize(dataset):\n",
    "    t1 = time()\n",
    "    arrays = [d['array'] for d in dataset['audio'][:]]\n",
    "    sampling_rates = [d['sampling_rate'] for d in dataset['audio'][:]]\n",
    "\n",
    "    inputs = processor(arrays, sampling_rate=sampling_rates[0], return_tensors=\"pt\", padding=True)\n",
    "    generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    transcriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print()\n",
    "    \n",
    "    speeches = synthesiser(transcriptions, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "    t2 = time()\n",
    "    run_time = (t2 - t1) / len(dataset['audio'])\n",
    "    print(f'Avg run time per audio {np.round(run_time, 4)}s')\n",
    "    \n",
    "    ret_files = []\n",
    "    for i, s in enumerate(speeches):\n",
    "        name = config['save_dir']+f'/speech_anon_{i}.wav'\n",
    "        sf.write(name, s[\"audio\"], samplerate=s[\"sampling_rate\"])\n",
    "        ret_files.append(name)\n",
    "\n",
    "    return ret_files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "12413c11-2339-403c-b812-5da40b98fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg run time per audio 4.1784s\n"
     ]
    }
   ],
   "source": [
    "anon_files = anonimize(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecf7ed-c5a0-4b30-b04c-d612c2ce7522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "960036aa-7913-40e5-8128-7e12d009b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer( orig_src, anon_src ):\n",
    "    inference = Inference(model_emb, window=\"whole\")\n",
    "    \n",
    "    orig_embeddings = [\n",
    "        inference(f) for f in orig_src    \n",
    "    ]\n",
    "\n",
    "    anon_embeddings = [\n",
    "        inference(f) for f in anon_src \n",
    "    ]\n",
    "\n",
    "    emb_list = orig_embeddings + anon_embeddings\n",
    "    label_list = [1]*len(orig_embeddings) + [-1]*len(anon_embeddings)\n",
    "\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    \n",
    "    for emb1, label1 in zip(emb_list, label_list):\n",
    "        for emb2, label2 in zip(emb_list, label_list):\n",
    "            distance = cdist(emb1.reshape(1,-1), emb2.reshape(1,-1), metric=\"cosine\")[0,0]\n",
    "            score = max(0, 1-distance)\n",
    "            if label1!=label2:\n",
    "                negative_scores.append(score)\n",
    "            else:\n",
    "                positive_scores.append(score)\n",
    "\n",
    "    #print(positive_scores, negative_scores)\n",
    "    val_eer, threshold = EER(torch.tensor(positive_scores), torch.tensor(negative_scores))\n",
    "\n",
    "    return val_eer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7e0861a4-517e-453e-ac71-fdf6165fb6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_eer(\n",
    "    [a['path'] for a in ds[\"audio\"]], \n",
    "    anon_files[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68439dc-95cc-4f97-abea-fc6f260b0c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41caa27a-79e3-46cf-96ab-2c351cb2e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer( orig_texts, anon_paths ):\n",
    "    metric = WordErrorRate()\n",
    "\n",
    "    #Load and stt anon\n",
    "    anon_arrs = []\n",
    "    anon_rates = []\n",
    "    for p in anon_paths:\n",
    "        arr, rate = sf.read(p)\n",
    "        anon_arrs.append(arr)\n",
    "        anon_rates.append(rate)\n",
    "    inputs = processor(anon_arrs, sampling_rate=anon_rates[0], return_tensors=\"pt\", padding=True)\n",
    "    generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    anon_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for org, an in zip(orig_texts, anon_texts):\n",
    "        metric.update([an.lower()], [org.lower()])\n",
    "\n",
    "    return metric.compute().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "58a2b915-7b5f-4e2e-89d2-eac0625f1a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08173912763595581"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_wer(ds['text'], anon_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac90847-b3a2-4ea4-a28e-7a1e2f56c254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d99d0e-92f6-40ec-9374-4175770e66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
